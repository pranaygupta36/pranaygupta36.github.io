<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Pranay Gupta</title>
  
  <meta name="author" content="Pranay Gupta">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/pranay_new.jpg">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Pranay Gupta</name>
              </p>
              <p>I am a PhD student at the Robotics Institute at Carnegie Mellon University, where I am advised by <a href="https://www.hennyadmoni.com">Prof. Henny Admoni</a>. I also collaborate with <a href="https://www.cs.cmu.edu/~abajcsy/">Prof. Andrea Bajcsy</a>.
              </p>
              <p> During the summer of 2025, I was a Research Intern at <a href="https://www.tri.global/">Toyota Research Institute</a> with the Human Interactive Driving Team. Previously, I was an MSR student at the Robotics Institute at Carnegie Mellon University, co-advised by <a href="https://www.hennyadmoni.com">Prof. Henny Admoni</a> and <a href="https://www.davheld.github.io">Prof. David Held</a>. Before that, I was a PreDoc Apprentice at TCS Research, working under <a href="https://in.linkedin.com/in/ramya-hebbalaguppe-620b272">Ms. Ramya Hebbalaguppe</a> and <a href="https://www.cse.iitd.ac.in/~narain/">Dr. Rahul Narain</a>. I completed my undergraduate studies in computer science at <a href="https://iiit.ac.in">IIIT Hyderabad</a>, where I worked as a research assistant at <a href="https://cvit.iiit.ac.in">CVIT</a> under the guidance of <a href="https://ravika.github.io">Prof. Ravi Kiran Sarvadevabhatla</a>.
              </p>
              <p> As a part of my <a href="https://www.ri.cmu.edu/publications/estimating-object-importance-and-modeling-drivers-situational-awareness-for-intelligent-driving/">Master's thesis</a> at CMU, I developed a method for real-time driver's Situational Awareness (SA) estimation using their eye-gaze. Prior to this, I worked on the problem of estimating an object's importance for making a safe driving decision for the ego vehicle.  At TCS, I worked on the problem of 3-D single view reconstruction. At CVIT, I worked on problems related to skeleton based action recognition and zero shot and generalised zero shot skeleton action recognition. I spent the summer of 2020 working as an Applied Scientist intern at Amazon India, where I worked on semantic text similarity using Bert based siamese networks. I have also worked under the supervision of <a href="https://www.microsoft.com/en-us/research/people/gmanish/"> Dr. Manish Gupta </a> on Knowledge aware video question answering. 
              </p>
              <p style="text-align:center">
                <a href="mailto:pranaygu@andrew.cmu.edu">Email</a> &nbsp/&nbsp
                <a href="data/pranay_resume.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=CL_es2wAAAAJ&hl=en&oi=ao">Google Scholar</a> &nbsp/&nbsp
                <a href="https://x.com/pranaygupta36">Twitter</a> &nbsp/&nbsp
                <a href="https://github.com/pranaygupta36/">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/pranay_new.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/pranay_new.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
              <p>
                My current research interests revolve around Human Robot Interaction, Shared Control for Robotics, Learning for Robotics and Assitive and Autonomous driving. I also have a keen interest in Computer Vision, 3D Computer Vision, Multimodal Learning and NLP. 
              </p>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px 20px 10px 20px;width:100%;vertical-align:middle">
              <subheading>Publications</subheading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr class="research-item">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <img src='images/corl25_analogy.png'>
              </div>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a>
                <papertitle>Adapting by Analogy: OOD Generalization of Visuomotor Policies via Functional Correspondence</papertitle>
              </a>
              <br>
              <strong>Pranay Gupta</strong>,
              <a href="https://www.hennyadmoni.com">Henny Admoni</a>,
              <a href="https://www.cs.cmu.edu/~abajcsy/">Andrea Bajcsy</a>
              <br>
              <em>9th Annual Conference on Robot Learning (CoRL 2025)</em>
              <br>
              <a href="https://arxiv.org/abs/2506.12678">Paper</a> /
              <a href="https://adapting-by-analogy.github.io/project-page/">Project Page</a>
              <details class="abstract">
                <summary>Abstract</summary>
                <p>End-to-end visuomotor policies trained using behavior cloning have shown a remarkable ability to generate complex, multi-modal low-level robot behaviors. However, at deployment time, these policies still struggle to act reliably when faced with out-of-distribution (OOD) visuals induced by objects, backgrounds, or environment changes. Prior works in interactive imitation learning solicit corrective expert demonstrations under the OOD conditions---but this can be costly and inefficient. We observe that task success under OOD conditions does not always warrant novel robot behaviors. In-distribution (ID) behaviors can directly be transferred to OOD conditions that share functional similarities with ID conditions. For example, behaviors trained to interact with in-distribution (ID) pens can apply to interacting with a visually-OOD pencil. The key challenge lies in disambiguating which ID observations functionally correspond to the OOD observation for the task at hand. We propose that an expert can provide this OOD-to-ID functional correspondence. Thus, instead of collecting new demonstrations and re-training at every OOD encounter, our method: (1) detects the need for feedback by checking if current observations are OOD and the most similar training observations show divergent behaviors (2) solicits functional correspondence feedback to disambiguate between those behaviors, and (3) intervenes on the OOD observations with the functionally corresponding ID observations to perform deployment-time generalization. We validate our method across diverse real-world robotic manipulation tasks with a Franka Panda robotic manipulator. Our results show that test-time functional correspondences can improve the generalization of a vision-based diffusion policy to OOD objects and environment conditions with low feedback.</p>
              </details>
            </td>
          </tr>

          <tr class="research-item">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <img src='images/corl24_driversa.png'>
              </div>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a>
                <papertitle>Modeling Drivers' Situational Awarenessfrom Eye Gaze for Driving Assistance</papertitle>
              </a>
              <br>
              <a href="https://www.cs.cmu.edu/~abhijatb/">Abhijat Biswas</a>,
              <strong>Pranay Gupta</strong>,
              <a href="https://www.pranaygupta36.github.io">Shreeya Khurana</a>,
              <a href="https://www.davheld.github.io">David Held</a>,
              <a href="https://www.hennyadmoni.com">Henny Admoni</a>,
              <br>
              <em>8th Annual Conference on Robot Learning (CoRL 2024)</em>
              <br>
              <a href="https://openreview.net/pdf?id=Occ1PVWC9p">Paper</a>
              <details class="abstract">
                <summary>Abstract</summary>
                <p>Intelligent driving assistance can alert drivers to objects in their environment; however, such systems require a model of drivers' situational awareness (SA) (what aspects of the scene they are already aware of) to avoid unnecessary alerts. Moreover, collecting the data to train such an SA model is challenging: being an internal human cognitive state, driver SA is difficult to measure, and non-verbal signals such as eye gaze are some of the only outward manifestations of it. Traditional methods to obtain SA labels rely on probes that result in sparse, intermittent SA labels unsuitable for modeling a dense, temporally correlated process via machine learning. We propose a novel interactive labeling protocol that captures dense, continuous SA labels and use it to collect an object-level SA dataset in a VR driving simulator. Our dataset comprises 20 unique drivers' SA labels, driving data, and gaze (over 320 minutes of driving) which will be made public. Additionally, we train an SA model from this data, formulating the object-level driver SA prediction problem as a semantic segmentation problem. Our formulation allows all objects in a scene at a timestep to be processed simultaneously, leveraging global scene context and local gaze-object relationships together. Our experiments show that this formulation leads to improved performance over common sense baselines and prior art on the SA prediction task.</p>
              </details>
            </td>
          </tr>

          <tr class="research-item">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <img src='images/oiecr.JPG'>
              </div>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a>
                <papertitle>Object Importance Estimation using Counterfactual Reasoning for Intelligent Driving</papertitle>
              </a>
              <br>
              <strong>Pranay Gupta</strong>,
              <a href="https://www.cs.cmu.edu/~abhijatb/">Abhijat Biswas</a>,
              <a href="https://www.hennyadmoni.com">Henny Admoni</a>,
              <a href="https://www.davheld.github.io">David Held</a>
              <br>
              <em>IEEE RA-L and ICRA 2025</em>
              <br>
              <a href="https://arxiv.org/abs/2312.02467">Paper</a> /
              <a href="https://vehicle-importance.github.io/">Project Page</a> /
              <a href="https://github.com/vehicle-importance/oiecr">Code</a>
              <details class="abstract">
                <summary>Abstract</summary>
                <p>The ability to identify important objects in a complex and dynamic driving environment is essential for autonomous driving agents to make safe and efficient driving decisions. It also helps assistive driving systems decide when to alert drivers. We tackle object importance estimation in a data-driven fashion and introduce HOIST - Human-annotated Object Importance in Simulated Traffic. HOIST contains driving scenarios with human-annotated importance labels for vehicles and pedestrians. We additionally propose a novel approach that relies on counterfactual reasoning to estimate an object's importance. We generate counterfactual scenarios by modifying the motion of objects and ascribe importance based on how the modifications affect the ego vehicle's driving. Our approach outperforms strong baselines for the task of object importance estimation on HOIST. We also perform ablation studies to justify our design choices and show the significance of the different components of our proposed approach.</p>
              </details>
            </td>
          </tr>

          <tr class="research-item">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <img src='images/pakdd22.JPG'>
              </div>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a>
                <papertitle>News KVQA:Knowledge-Aware News Video Question Answering</papertitle>
              </a>
              <br>
              <strong>Pranay Gupta</strong>,
              <a href="https://www.microsoft.com/en-us/research/people/gmanish/">Manish Gupta</a>
              <br>
              <em>PAKDD</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2202.04015">Paper</a>
              <details class="abstract">
                <summary>Abstract</summary>
                <p>In this paper, we explore knowledge-based question answering in the context of news videos. To this end, we curate a new dataset with over 1M multiple-choice question-answer pairs. Using this dataset, we propose a novel approach, NEWSKVQA (Knowledge-Aware News Video Question Answering) which performs multi-modal inferencing over textual multiple-choice questions, videos, their transcripts and knowledge base</p>
              </details>
            </td>
          </tr>
          
          <tr class="research-item">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <img src='images/quovadis-overview.jpg'>
              </div>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://skeleton.iiit.ac.in/">
                <papertitle>Quo Vadis, Skeleton action recognition?</papertitle>
              </a>
              <br>
              <strong>Pranay Gupta</strong>,
              <a href="https://anirudh257.github.io/">Anirudh Thatipelli</a>,
              <a href="https://in.linkedin.com/in/aditya-aggarwal-21343a74">Aditya Aggarwal</a>,
              <a href="https://in.linkedin.com/in/shubh-maheshwari-663737151">Shubh Maheshwari</a>,
              <a href="https://in.linkedin.com/in/neel-trivedi-7a486b193">Neel Trivedi</a>, <br>
              <a href="https://github.com/SodaCoder">Sourav Das</a>,
              <a href="https://ravika.github.io">Sarvadevabhatla, Ravi Kiran</a>
              <br>
              <em>IJCV, Special Issue on Human pose, Motion, Activities and Shape in 3D</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2007.02072">Paper</a> /
              <a href="https://skeleton.iiit.ac.in/">Project Page</a> /
              <a href="https://github.com/skelemoa/quovadis">Code</a>
              <details class="abstract">
                <summary>Abstract</summary>
                <p>In this paper, we study current and upcoming frontiers across the landscape of skeleton-based human action recognition. We introduce skeletics-152, a large scale into-the-wild skeleton action dataset. We extend out analysis to out of context actions by introducing Skelton-Mimetics dataset. Finally we introduce Metaphorics, a dataset with caption-style annotated YouTube videos of the popular social game Dumb Charades and interpretative dance performances. We benchmark state-of-the-art models on the NTU-120 dataset and provide multi-layered assessment of the results.</p>
              </details>
            </td>
          </tr>

          <tr class="research-item">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <img src='images/synse-overview.png'>
              </div>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://skeleton.iiit.ac.in/zeroshotlearning">
                <papertitle>Syntactically Guided Generative Embeddings for Zero-Shot Skeleton Action Recognition</papertitle>
              </a>
              <br>
              <strong>Pranay Gupta</strong>,
              <a href="https://divyanshusharma1709.github.io/">Divyanshu Sharma</a>,
              <a href="https://ravika.github.io">Sarvadevabhatla, Ravi Kiran</a>
              <br>
              <em>ICIP</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2101.11530">Paper</a> /
              <a href="https://skeleton.iiit.ac.in/zeroshotlearning">Project Page</a> /
              <a href="https://github.com/skelemoa/synse-zsl">Code</a>
              <details class="abstract">
                <summary>Abstract</summary>
                <p>In this paper, we study the effect of learning Part of Speech aware generative embeddings for zero shot and generalised zero shot skelton action recognition.</p>
              </details>
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:20px 20px 10px 20px;width:100%;vertical-align:middle">
              <subheading>Workshop Papers</subheading>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr class="research-item">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <img src='images/vamhri24.png'>
              </div>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a>
                <papertitle>An Interactive Protocol to Measure a Driver's Situational Awareness</papertitle>
              </a>
              <br>
              <a href="https://www.cs.cmu.edu/~abhijatb/">Abhijat Biswas*</a>,
              <strong>Pranay Gupta*</strong>,
              <a href="https://www.davheld.github.io">David Held</a>,
              <a href="https://www.hennyadmoni.com">Henny Admoni</a>
              <br>
              <em>7th International Workshop on Virtual, Augmented, and Mixed-Reality for Human-Robot Interactions (VAM-HRI)</em>
              <br>
              <a href="https://openreview.net/pdf?id=Occ1PVWC9p">Paper</a>
              <details class="abstract">
                <summary>Abstract</summary>
                <p>Commonly used protocols for capturing the ground-truth situational awareness (SA) of drivers involve halting a simulation and querying the driver. SA data collected in this way is unsuitable for training models for predicting real-time SA since it is inherently intermittent and does not capture transitions of SA (e.g. from not aware to aware). We introduce an efficient VR based interactive protocol designed to capture a driver's ground-truth situational awareness (SA) in real time. Our protocol mitigates the aforementioned limitations of prior approaches, and allows capturing continuous object-level SA labels that are more suitable for downstream real-time SA prediction tasks. Our initial findings highlight its potential as a scalable solution for curating large scale driving datasets with ground-truth SA.</p>
              </details>
            </td>
          </tr>

          <tr class="research-item">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <img src='images/hllm_hri24.png'>
              </div>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a>
                <papertitle>Leveraging Vision and Language Models for Zero-Shot, Personalization of Household Multi-Object Rearrangement Tasks</papertitle>
              </a>
              <br>
              <a href="https://www.andrew.cmu.edu/user/bnewman1/">Benjamin A. Newman</a>,
              <strong>Pranay Gupta</strong>,
              <a href="https://talkingtorobots.com/yonatanbisk.html">Yonatan Bisk</a>,
              <a href="https://kriskitani.github.io/">Kris Kitani</a>,
              <a href="https://www.hennyadmoni.com">Henny Admoni</a>,
              <a href="https://cpaxton.github.io/">Chris Paxton</a>
              <br>
              <em>HRI 24' Workshop on Human â€“ Large Language Model Interaction</em>
              <br>
              <a href="https://human-llm-interaction.github.io/workshop/hri24/papers/hllmi24_paper_9.pdf">Paper</a>
              <details class="abstract">
                <summary>Abstract</summary>
                <p>Robots should adhere to personal preferences when performing household tasks. Many household tasks can be posed as multi-object rearrangement tasks, but solutions to these problems often target a single, hand defined solution or are trained to match a solution drawn from a distribution of human demonstrated data. In this work, we consider using an internet-scale pre-trained vision-andlanguage foundation model as the backbone of a robot policy for producing personalized task plans to solve household multi-object rearrangement tasks. We present initial results on a one-step table setting task that shows a proof-of-concept for this method.</p>
              </details>
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Service</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="10"><tbody>
          <tr>
            <td style="padding:12px 20px;">
              <strong>Reviewer:</strong> IEEE RA-L 2025, IEEE ICRA 2026, ACM HRI 2025, CoRL 2025
            </td>
          </tr>
          <tr>
            <td style="padding:12px 20px;">
              <strong>Teaching Assistant:</strong> Human Robot Interaction (Fall 2024), Intro to Human Robot Interaction (Spring 2023), Computer Vision (Spring 2020)
            </td>
          </tr>
          </tbody>
        </table>

        <br>
        <br>
        <br>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:center;font-size:small;">
                Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's website</a>
              </p>
            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
</body>

</html>
